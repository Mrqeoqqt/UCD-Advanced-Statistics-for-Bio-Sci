---
title: "STA 101 R Handout 2 - Diagnostics, HT, CI"
author: "Erin K. Melcon"
date: "April 17, 2017"
output: html_document
---
### Load these packages
For this handout, we need to load the package `MASS`, which is already installed in R.  To do this, type `library(MASS)` in your console. 

### The data
We will continue to use the `cats` dataset, so you'll need to load the library `MASS`:
```{r}
library(MASS)
head(cats)
```

### 1. Identifying Outliers
There are a number of ways to identify outliers, but one of them is to look at the scatter plot and find any unusual observations.  I'll use the ugly plot for plotting all values of $Y$ against all values of $X$:

```{r}
plot(cats)
```

We want to look at the plots that have $Y = $ Bwt against another variable, and see if there are any unusual observations.  There does appear to be a high heart weight value, which roughly is something above 18g.  Using this, and the subsetting from last week, we can identify the outlier "row" as:

```{r}
cats[which(cats$Hwt > 18),]
```

The general format, once you have figured out which column you are looking at, and if the value is above or below a certain reference point is:

`dataset[which(dataset$columnname > reference),]` (for high outliers)
`dataset[which(dataset$columnname < reference),]` (for low outliers)

Later, we will remove the outliers, but for now I'll use the different methods to identify them.

### 2 Assessing Normality
For the rest of the handout, it will be useful to add two columns to our dataset - one for the errors and one for the fitted values.  Once you have a model fitted (I'll assume you've called it `the.model`), the generic commands to do that are:

`dataset$ei = the.model$residuals`
`dataset$yhat = the.model$fitted.values`

For example,

```{r}
the.model = lm(Bwt ~ Hwt + Sex,data = cats)
cats$ei = the.model$residuals
cats$yhat = the.model$fitted.values
```

There are two primary ways to assess normality.

### 2.1 QQ plot
we can easily obtain the qqplot with the following generic commands:

`qqnorm(the.model$residuals)`
`qqline(the.model$residuals)`

The second line adds the reference point of $y=x$.  For example, our QQ plot of our data is:
```{r}
qqnorm(the.model$residuals)
qqline(the.model$residuals)
```

This plot has most of the points very close to the line, with one or two possible outliers.  For example, I can look at the y axis and try and eyeball what value is an outlier, then use subsetting to display that row.  I think anything larger than 0.70 is an outlier, so to subset that I can do:

```{r}
cats[which(cats$ei > 0.7),]
```
### 2.2 Testing Normality (Shapiro-Wilks).
To formally test $H_0$: The errors are normally distributed vs. $H_A$: The errors are not normally distributed, we can use the function `shapiro.test`.  Assuming you've saved your linear model, we can easily get the p-value back for the test:
```{r}
ei = the.model$residuals
the.SWtest = shapiro.test(ei)
the.SWtest
```
I can print out all the information, but I can also just reference the p-value if I have saved the information (as I did above).  The p-value for our test was :` `r the.SWtest$p.value`

Since our p-value was relatively large, we fail to reject the null, and support that our data is normally distributed at any reasonable significance level (1%, 5%, 10%). 

### 3 Assessing constant variance (homoscedasticity)
Again, there are two main ways of assessing homoscedasticity.

### 3.1 Plotting errors vs. fitted values
A simple plot can reveal large patterns.  The generic commands are:

`plot(the.model$fitted.values, the.model$residuals, main = "Errors vs. Fitted Values",xlab = "Fitted Values",ylab = "Errors")`
`abline(h = 0,col = "purple")`

The second line adds a purple line at $ei=0$. 

To use ggplot2 nicely, I'm assuming you've added the columns with the errors and fitted values. Then, the generic command is:

`qplot(yhat, ei, data = dataset) +  ggtitle("Errors vs. Fitted Values") + xlab("Fitted Values") + ylab("Errors") + geom_hline(yintercept = 0,col = "purple")`

For example, in our `cats` dataset:

```{r} 
library(ggplot2)
qplot(yhat, ei, data = cats) +  ggtitle("Errors vs. Fitted Values") + xlab("Fitted Values") + 
  ylab("Errors") + geom_hline(yintercept = 0,col = "purple")
```

Again, we see some possible outliers, and again I can find them using either values of `ei`, or `yhat`:

```{r}
cats[which(cats$yhat > 4),]
```

### 3.2 Fligner Killeen test (FK test).
For this test, we have to create a vector for grouping the data, where group 1 is anything less than or equal to the median of the response variable (or fitted values), and group 2 is anything above the response variable. 

The generic commands to do this, and then add the column to the dataset are:

`Group = rep("Lower",nrow(dataset))` #Creates a vector that repeats "Lower" n times
`Group[dataset$Ycolumn < median(dataset$Ycolumn)] = "Upper"` #Changing the appropriate values to "Upper"
`Group = as.factor(Group)` #Changes it to a factor, which R recognizes as a grouping variable.
`dataset$Group = Group`

Now, we can use the function `fligner.test` to find the p-value for $H_0$:  There are equal variances for the upper and lower groups vs. $H_A$: There are unequal variance between the upper and lower groups.  The generic commands to do this are:

`fligner.test(dataset$ei, dataset$Group)`

For example, in our cats dataset we could do:

```{r}
cats$ei = the.model$residuals #If you have not done this already, do this now.
Group = rep("Lower",nrow(cats)) #Creates a vector that repeats "Lower" n times
Group[cats$Bwt < median(cats$Bwt)] = "Upper" #Changing the appropriate values to "Upper"
Group = as.factor(Group) #Changes it to a factor, which R recognizes as a grouping variable.
cats$Group = Group
the.FKtest= fligner.test(cats$ei, cats$Group)
the.FKtest
```
Then, we can find the p-value is: `r the.FKtest$p.value`.  This is still larger than any typical alpha, so we would support that the lower variance is equal to the upper variance.

### 4 Additional outlier methods
Additional methods of identifying outliers are using standardized errors (so that they are Z ~ N(0,1), and then outliers are anything above 3 or below -3), or by using Cooks Distance.

### 4.1 Cooks distance
To calculate Cook's Distance, we can use the function `cooks.distance`.  The generic command is:

`CD = cooks.disance(the.model)`

Then, to identify any that are over some `cutoff`

`CD[CD > cutoff]`

Note, the above may not return 

For example, in our dataset:
```{r}
cutoff = 0.20
CD = cooks.distance(the.model)
CD[CD > cutoff]
```
To visualize the Cook's Distance, we can easily plot them with the following generic command:

`plot(CD,ylab = "Cook's Distance")`
`abline(h = cutoff,color = "purple")`

Note, if your cutoff is too high (that is fine) it may not show up on the graph. 

### Choosing Cutoffs based on t distribution (for standardized residuals)
If you want to use a percentile of the t-distibution to find your cutoff, you will need $n$ and $p$.  We can get these in R:
```{r}
n = length(the.model$residuals) #Counts the number of ei values (which should be n)
p = length(the.model$coefficients) # Counts the number of betas
```

Then, the general command for a $(1-\alpha/2)100^{th}$ percentile is:\\
```{r}
alpha = 0.01 # You may change this to whatever you like
t.cutoff = qt(1- alpha/2, n-p)
t.cutoff
```

We will use this later.  You can also replace $\alpha/2$ with $\alpha/(2n)$, which would give a much higher value for the cutoff.  For example:

```{r}
qt(1-alpha/(2*n), n-p)
```

### 4.2 Standardized residuals 
The code to calculate $e_i^\ast$ follows:

`ei.s = the.model$residuals/sqrt(sum(the.model$residuals^2)/(length(the.model$residuals) - length(the.model$coefficients)))`

All you would potentially need to replace in the above is `the.model`, if you called it something different. It is just calculating

$e_i^\ast = \frac{e_i}{\sqrt{MSE}}$

Using our t-cutoff from above, we have:

```{r}
ei.s = the.model$residuals/sqrt(sum(the.model$residuals^2)/(length(the.model$residuals) - length(the.model$coefficients)))
outliers = which(abs(ei.s) > t.cutoff)
outliers
```

### 4.3 Standardized residuals 
The standardized residuals can be extracted with the following generic command:

`SR = stdres(the.model)`

And then we could make a histogram for them, or look for some above and beyond a certain cutoff:
`hist(SR, main = "Standardized residuals")`
`SR[SR > cutoff |  SR < -cutoff]`

For example, lets say my cutoff is -3.  Then, my standardized residuals are:

```{r}
library(MASS) #Load the required library if you have not already.
SR = stdres(the.model)
cutoff= 3
hist(SR,main = "Standardized residuals")
SR[abs(SR) > cutoff] # Show all above the poistive cutoff or below the -cutoff 
```

If we use the t-cutoff above, we have:

```{r}
outliers = which(abs(SR)>t.cutoff)
outliers
```
This is stating that those **rows** contain outliers. 

Once we have identified outliers, we may want to remove them.  

**If you want to remove few outliers, you should make your cutoff larger by using a smaller $\alpha$ or by using $\alpha/(2n)$.**

### 5 Removing outliers from a dataset
Want we need in order to remove outliers is **the row number the outlier was in**.  Fortunately, this is what most of my code does - finds the row number of a suspect outlier.

Then, once you have the row number (or a list of row numbers), call it `outliers`, we can remove them by the following:

`new.data = dataset[-outliers,]`

For example, lets say I use the standardized residuals to find outliers.  Then, I can save those that are outliers as follows:

```{r}
outliers = which(SR > cutoff |  SR < -cutoff)
new.data = cats[-outliers,]
```

Now, I may want to refit my regression model in order to remove the influence of the outliers.  The only thing that would change is you would use `new.data` rather than `dataset`.

```{r}
new.model = lm(Bwt ~ Hwt + Sex,data = new.data)
```

### 5 Confidence intervals and Hypothesis Tests for $\beta$

I'm assuming that you have already fit a linear model.

### 5.1 Confidence intervals

To find confidence intervals for every $\beta$, the generic format of the code is:

`alpha = ` #Here you would put 0.10, 0.05, 0.01
`confint(the.model, level = 1-alpha)`

For example, if we wanted 95\% confidence intervals for the `new.model`:
```{r}
alpha = 0.05
the.CIs = confint(new.model,level = 1-alpha)
the.CIs
```
Notice this has (in this problem) 3 rows, and two columns.  To extract the `i`th row, we would use the following command:

`the.CIs[i,]`

For example, if we wanted the confidence interval for $\beta_1$:
```{r}
the.CIs[2,]
```
Or, the 95\% confidence interval is: (`r the.CIs[2,1]`, `r the.CIs[2,2]`).

### 5.2 Hypothesis tests for a single beta

There is also a very straightforward way to get the standard errors, test-statistics, and p-values for testing $H_0: \beta_i = 0$.  The generic command is:

`summary(the.model)`

This gives quite a bit of information, and if we only want the test-statistic information, we would use:

`summary(the.model)$coefficients`

The first column is the values of $\hat{\beta}_i$, the next $SE(\hat{\beta}_i)$, the next the test-statistic, and the last the value of the two-sided p-value.

For example,

```{r}
test.stuff = summary(new.model)$coefficients
summary(new.model)$coefficients
```

From the above all of the $\beta$'s are significant, since the p-values are all very small.

### 5.3 Testing a large and small model

When we have a "larger model" vs a "smaller model", we either want to fit two ANOVA tables, or there is a function that will allow us to compare the models directly.  You'll want to fit two models, one with less $X$ variables in it, and one that has more.  

For example, consider the Guinea Pig data from lecture, which can be found in the dataset `ToothGrowth`.  Maybe we want to test if the interaction term, and the delivery method should be dropped from the model.

Fitting the two models with their ANOVA tables give:

```{r}
smaller.model = lm(len ~ dose, data = ToothGrowth)
anova.small = anova(smaller.model)
larger.model = lm(len ~ dose + supp + dose*supp, data = ToothGrowth)
anova.large = anova(larger.model)
```

Now, the generic command to test $H_0$:  The smaller model fits better (i.e $\beta_2 = \beta_3 = 0$ in our example), vs. $H_A$:  The larger model fits better (at least one $\beta_2, \beta_3 \neq 0$ in our example) is:

`anova(smaller.model,larger.model)`

This gives the separate values of SSE, the test-statistic, and the p-value:

```{r}
anova(smaller.model,larger.model)
```
The first row gives the degrees of freedom for SSE for the smaller model, and the SSE for the smaller model.  The next gives the degrees of freedom for SSE for the larger model, the SSE for the larger model, the difference in the SSE's, the value of the F statistic, and the corresponding p-value.


